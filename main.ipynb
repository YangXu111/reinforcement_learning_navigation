{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The current device is cuda:0\n",
      "2021-02-04 21:40:06,650 INFO CUDA version: 11.0, CUDA enabled: True\n"
     ]
    }
   ],
   "source": [
    "# import IPython\n",
    "# IPython.Application.instance().kernel.do_shutdown(True) #automatically restarts kernel\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from unityagents import UnityEnvironment\n",
    "import matplotlib.pyplot as plt\n",
    "from dqn_agent import DQNAgent\n",
    "from double_dqn_agent import DoubleDQNAgent\n",
    "from prioritized_replay_dqn_agent import PrioritizedReplayDQNAgent\n",
    "from dueling_dqn_agent import DuelingDQNAgent\n",
    "import sys, math, time, torch, logging, json\n",
    "\n",
    "# os.environ['CUBLAS_WORKSPACE_CONFIG'] = ':4096:8'\n",
    "log_file_path = 'output.log'\n",
    "result_file_path = 'result.json'\n",
    "algorithm = 'DQN'\n",
    "# ['DQN','Double DQN', 'Prioritized Experience Replay', 'Dueling DQN']\n",
    "\n",
    "logger = logging.getLogger('p1_navigation')\n",
    "while logger.handlers:\n",
    "    logger.removeHandler(logger.handlers[0])\n",
    "logger.propagate = False\n",
    "\n",
    "formatter = logging.Formatter('%(asctime)s %(levelname)s %(message)s')\n",
    "\n",
    "sh = logging.StreamHandler(sys.stdout)\n",
    "sh.setFormatter(formatter)\n",
    "sh.setLevel(logging.DEBUG)\n",
    "\n",
    "fh = logging.FileHandler(log_file_path)\n",
    "fh.setLevel(logging.INFO)\n",
    "\n",
    "logger.addHandler(sh)\n",
    "logger.addHandler(fh)\n",
    "logger.setLevel(logging.DEBUG)\n",
    "\n",
    "\n",
    "logger.info('CUDA version: %s, CUDA enabled: %s' % (torch.version.cuda, torch.backends.cudnn.enabled))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = UnityEnvironment(file_name='Banana_Windows_x86_64\\Banana.exe')\n",
    "# get the default brain\n",
    "brain_name = env.brain_names[0]\n",
    "brain = env.brains[brain_name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reset the environment\n",
    "env_info = env.reset(train_mode=True)[brain_name]\n",
    "\n",
    "# number of agents in the environment\n",
    "logger.debug('Number of agents: %i' % len(env_info.agents))\n",
    "\n",
    "# examine the state space \n",
    "state = env_info.vector_observations[0]\n",
    "logger.debug('States look like: %s' % state)\n",
    "state_size = len(state)\n",
    "\n",
    "# number of actions\n",
    "action_size = brain.vector_action_space_size\n",
    "logger.info('Number of agents: %i, states %i, actions: %i' % (len(env_info.agents), state_size, action_size))\n",
    "\n",
    "if algorithm == 'DQN':\n",
    "    from constants import CONSTANTS as C\n",
    "    agent = DQNAgent(state_size, action_size)\n",
    "elif algorithm == 'Double DQN':\n",
    "    from constants import CONSTANTS as C\n",
    "    agent = DoubleDQNAgent(state_size, action_size)\n",
    "elif algorithm == 'Prioritized Experience Replay':\n",
    "    from constants import PRIORITIZED_REPLAY_CONSTANTS as C\n",
    "    agent = PrioritizedReplayDQNAgent(state_size, action_size)\n",
    "    beta = C['beta_begin']\n",
    "    logger.info('It takes %f steps for beta to go from %f to %f' % ((C['beta_stable']-C['beta_begin'])/C['beta_increase'], C['beta_begin'], C['beta_stable']))\n",
    "elif algorithm == 'Dueling DQN':\n",
    "    from constants import CONSTANTS as C\n",
    "    agent = DuelingDQNAgent(state_size, action_size)\n",
    "else:\n",
    "    logger.warning('No algorithm specified')\n",
    "\n",
    "logger.info(algorithm)\n",
    "logger.info(json.dumps(C, indent=4))\n",
    "\n",
    "episode_score_hist = []\n",
    "total_score = 0\n",
    "\n",
    "epsilon = C['epsilon_begin']\n",
    "logger.info('It takes %f steps for epsilon to go from %f to %f' % (math.log(C['epsilon_stable']/C['epsilon_begin'], C['epsilon_decay']), C['epsilon_begin'], C['epsilon_stable']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "step = 0\n",
    "for i in range(C['num_episodes'] + 1):\n",
    "    env_info = env.reset(train_mode=True)[brain_name]  # reset the env at each episode\n",
    "    state = env_info.vector_observations[0]\n",
    "    done = False\n",
    "    episode_score = 0                                  # initialize the score\n",
    "    while not done:\n",
    "        if algorithm == 'Prioritized Experience Replay':\n",
    "            action = agent.action(state, epsilon=epsilon, beta=beta)          # select an action\n",
    "            beta = min(beta+C['beta_increase'], C['beta_stable'])\n",
    "        else:\n",
    "            action = agent.action(state, epsilon=epsilon)\n",
    "        env_info = env.step(action)[brain_name]        # send the action to the environment\n",
    "        next_state = env_info.vector_observations[0]   # get the next state\n",
    "        reward = env_info.rewards[0]                   # get the reward\n",
    "        done = env_info.local_done[0]                  # see if episode has finished\n",
    "        agent.replay_buffer.add(state, action, reward, next_state, done)\n",
    "        step += 1\n",
    "#         logger.info('step, action, reward:%i, %i, %i' % (step, action, reward))\n",
    "        episode_score += reward                        # update the score\n",
    "        state = next_state                             # roll over the state to next time step\n",
    "    total_score += episode_score\n",
    "    episode_score_hist.append(episode_score)\n",
    "    epsilon = max(epsilon*C['epsilon_decay'], C['epsilon_stable'])\n",
    "    if i % 100 == 0:\n",
    "        logger.info('For episode %i, the average score is %.2f, episode history %s' % (i, total_score/100, episode_score_hist[-100:]))\n",
    "        total_score = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.info('The training completes in %f mins' % ((time.time() - start_time)/60))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 10))\n",
    "plt.plot(episode_score_hist)\n",
    "plt.title('Episode Score with %s' % algorithm)\n",
    "plt.xlabel('Episodes')\n",
    "plt.ylabel('Scores')\n",
    "plt.savefig('episode_score_with_%s.png' % algorithm, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "C['agent'] = type(agent).__name__\n",
    "C['episode_score_hist'] = episode_score_hist\n",
    "with open(result_file_path, 'a', encoding='utf-8') as f:\n",
    "    json.dump(C, f, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "env_info = env.reset(train_mode=False)[brain_name] # reset the environment\n",
    "state = env_info.vector_observations[0]            # get the current state\n",
    "score = 0                                          # initialize the score\n",
    "while True:\n",
    "    action = agent.action(state, epsilon=epsilon, beta=0)                   # select an action\n",
    "    env_info = env.step(action)[brain_name]        # send the action to the environment\n",
    "    next_state = env_info.vector_observations[0]   # get the next state\n",
    "    reward = env_info.rewards[0]                   # get the reward\n",
    "    done = env_info.local_done[0]                  # see if episode has finished\n",
    "    score += reward                                # update the score\n",
    "    state = next_state                             # roll over the state to next time step\n",
    "    if done:                                       # exit loop if episode finished\n",
    "        break\n",
    "    \n",
    "logger.info(\"Score: {}\".format(score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "drlnd1",
   "language": "python",
   "name": "drlnd"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}